10.100.102.26

kubeadm init --control-plane-endpoint="10.100.102.31:8443" --upload-certs --apiserver-advertise-address=10.100.102.26 --pod-network-cidr=10.32.0.0/12


To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 10.100.102.31:8443 --token 4norro.fmjcx0td5hjezhnj \
        --discovery-token-ca-cert-hash sha256:7de9cf18390d4ebdb845a33876741e4b95794525a3a406ceb5072d2533858e66 \
        --control-plane --certificate-key a1d1d0335fb0219e2172f7d638c2635d4dfa9a07cca62842775adb363b8901bf

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.100.102.31:8443 --token 4norro.fmjcx0td5hjezhnj \
        --discovery-token-ca-cert-hash sha256:7de9cf18390d4ebdb845a33876741e4b95794525a3a406ceb5072d2533858e66
		
  kubeadm join 10.100.102.31:8443 --token 4norro.fmjcx0td5hjezhnj \
        --discovery-token-ca-cert-hash sha256:7de9cf18390d4ebdb845a33876741e4b95794525a3a406ceb5072d2533858e66 \
        --control-plane --certificate-key a1d1d0335fb0219e2172f7d638c2635d4dfa9a07cca62842775adb363b8901bf --apiserver-advertise-address=10.100.102.27

----------------------------------------------------------------------------------------------------------------------------------
vi haproxy.cfg

global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
        stats timeout 30s
        user haproxy
        group haproxy
        daemon

        # Default SSL material locations
        ca-base /etc/ssl/certs
        crt-base /etc/ssl/private

        # See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate
        ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
        ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

defaults
        log     global
        mode    http
        option  httplog
        option  dontlognull
        option http-server-close
        option forwardfor       except 127.0.0.0/8
        option                  redispatch
        retries                 1
        timeout http-request    10s
        timeout queue           20s
        timeout connect         5s
        timeout client          20s
        timeout server          20s
        timeout http-keep-alive 10s
        timeout check           10s
        #timeout connect 5000
        #timeout client  50000
        #timeout server  50000
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http

#---------------------------------------------------------------------
# apiserver frontend which proxys to the control plane nodes
#---------------------------------------------------------------------
frontend apiserver
    bind *:8443
    mode tcp
    option tcplog
    default_backend apiserver

#---------------------------------------------------------------------
# round robin balancing for apiserver
#---------------------------------------------------------------------
backend apiserver
    option httpchk GET /healthz
    http-check expect status 200
    mode tcp
    option ssl-hello-chk
    balance     roundrobin
        server k8smaster1 10.100.102.26:6443 check fall 3 rise 2
        server k8smaster2 10.100.102.27:6443 check fall 3 rise 2
        server k8smaster3 10.100.102.28:6443 check fall 3 rise 2

        # [...]

------------------------------------------------------------------------------------------------------------
vi keepalived.conf

global_defs {
    router_id LVS_DEVEL
}
vrrp_script check_apiserver {
  script "/etc/keepalived/check_apiserver.sh"
  interval 3
  weight -2
  fall 10
  rise 2
}

vrrp_instance VI_1 {
    state MASTER
    interface ens0
    virtual_router_id 51
    priority 101
    authentication {
        auth_type PASS
        auth_pass 42
    }
    virtual_ipaddress {
        10.100.102.31
    }
    track_script {
        check_apiserver
    }
}
-----------------------------------------------------------------------------------------------------
vi check_apiserver.sh


#!/bin/sh

errorExit() {
  echo "*** $@" 1>&2
  exit 1
}

curl --silent --max-time 2 --insecure https://localhost:6443/ -o /dev/null || errorExit "Error GET https://localhost:6443/"
if ip addr | grep -q 10.100.102.31; then
  curl --silent --max-time 2 --insecure https://10.100.102.31:6443/ -o /dev/null || errorExit "Error GET https://10.100.102.31:6443/"
fi

-----------------------------------------------------------------------------------------------------------------------
vi daemon.json

{
  "exec-opts": ["native.cgroupdriver=systemd"]
}


